{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e",
   "metadata": {},
   "source": [
    "# Notebook 3: Classification using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38388419-5a1e-458f-9a07-1179014901ba",
   "metadata": {},
   "source": [
    "**Only run it if you’re adopting or experimenting with this model, as it can take up to two hours to train.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cbf42-add7-486e-9e2d-6f04051b812e",
   "metadata": {},
   "source": [
    "In this notebook, we will use a CNN model to classify the images, following the approach used in the following [paper](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13095d08-82fa-49dc-8613-e8d4d1320555",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4",
   "metadata": {},
   "source": [
    "First, we’ll load the saved image and label data from the NumPy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4a8dd1-52ed-4514-8cd1-a59980fab45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from NumPy files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Importing NumPy for numerical operations and array handling\n",
    "\n",
    "# Load the images and labels back from the saved NumPy files\n",
    "train_images = np.load('train_images.npy')  # Load image training data\n",
    "val_images = np.load('val_images.npy')      # Load image validation data\n",
    "train_labels = np.load('train_labels.npy')  # Load label training data\n",
    "val_labels = np.load('val_labels.npy')      # Load label validation data\n",
    "\n",
    "print(\"Data loaded successfully from NumPy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953fc9a-d986-47e1-8872-213f8518cdb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a46294-7dfd-46c0-9146-d53220c58dbb",
   "metadata": {},
   "source": [
    "### Train CubeCatNet CNN mdoel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66741a35-2b50-4d26-a964-fc6d7da017d6",
   "metadata": {},
   "source": [
    "We will define and train a Convolutional Neural Network (CNN) model that was defined in [link](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7dbc5a-b183-4974-a3fd-7ce74c4fa66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 17:17:34.420777: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-28 17:17:34.427193: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-28 17:17:34.446322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-28 17:17:34.468651: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-28 17:17:34.476023: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-28 17:17:34.497044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-28 17:17:37.399505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/idia/projects/hack4dev/CubeSat_ImageClassify/vCube/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined and compiled successfully.\n",
      "Epoch 1/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 3s/step - accuracy: 0.7054 - loss: 0.9550\n",
      "Epoch 2/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 3s/step - accuracy: 0.9866 - loss: 0.0539\n",
      "Epoch 3/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 3s/step - accuracy: 0.9926 - loss: 0.0281\n",
      "Epoch 4/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 3s/step - accuracy: 0.9962 - loss: 0.0128\n",
      "Epoch 5/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 3s/step - accuracy: 0.9946 - loss: 0.0205\n",
      "Epoch 6/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 3s/step - accuracy: 0.9653 - loss: 0.1648\n",
      "Epoch 7/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 3s/step - accuracy: 0.9964 - loss: 0.0118\n",
      "Epoch 8/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 3s/step - accuracy: 0.9985 - loss: 0.0033\n",
      "Epoch 10/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 3s/step - accuracy: 0.9990 - loss: 0.0028\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # Importing Sequential to build the model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense  # Importing necessary layers for the CNN\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels (assuming you have 5 classes)\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "val_labels = to_categorical(val_labels, num_classes=5)\n",
    "\n",
    "\n",
    "# Define the CNN model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(512, 512, 3)),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(32, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(128, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    GlobalAveragePooling2D(),  # Global average pooling layer\n",
    "    Dense(5, activation='softmax')  # Output layer with 5 neurons (one for each class) + Softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate loss function, optimizer, and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model defined and compiled successfully.\")\n",
    "\n",
    "# Train the model on the training data\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=64,  # Batch size\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9d935-fda3-4ea3-8b33-4d5bd7cefc3a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e81814-27e8-4dbd-b73e-5976943b569a",
   "metadata": {},
   "source": [
    "### Perfomrmance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4ddbce-5789-4d82-88d7-5578e891a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 334ms/step\n",
      "(3237, 5)\n",
      "(3237, 5)\n",
      "Evaluation Metrics:\n",
      "evaluation_time: 37.37 seconds\n",
      "peak_memory_usage: 19525.90 MB\n",
      "average_cpu_usage: 1019.84%\n",
      "accuracy: 0.9975\n",
      "f1_score: 0.9980\n",
      "pipeline_size: 1.16 MB\n"
     ]
    }
   ],
   "source": [
    "from source.pre import evaluate_pipeline\n",
    "\n",
    "def preprocessing_fn(X):\n",
    "    \n",
    "    return X\n",
    "    \n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(model, val_images, val_labels, preprocessing_fn)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == 'evaluation_time':\n",
    "        print(f\"{key}: {value:.2f} seconds\")\n",
    "    elif key == 'pipeline_size':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'peak_memory_usage':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'average_cpu_usage':\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc867bfa-7257-4523-a593-6ee21c8a4f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CubeHackKer",
   "language": "python",
   "name": "cubehackker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
