{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac66dbc-7ba9-4115-98f7-f2470fb4b844",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94be4a-3bc6-4ee8-aee8-76ff5de0c030",
   "metadata": {},
   "source": [
    "In this notebook, we will compare the ML pipeline from notebook 3 with the Deep Learning pipeline from notebook 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec781abf-7ce3-4740-8a88-35e9aae2e2a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc76d53-6a3a-4508-9196-66e20ae4c16f",
   "metadata": {},
   "source": [
    "## Ensuring Fair Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad12b9-e716-48c6-8bd9-382ba2f1349c",
   "metadata": {},
   "source": [
    "This code sets the CPU affinity to core 0, ensuring that all threads and processes are restricted to a single CPU core. By doing so, you ensure consistent performance evaluation, as it prevents interference from other cores and provides a controlled environment for comparing methods on the same core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdbdaae3-0ce9-4121-aa07-9b0658f33712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "# Set CPU affinity to core 0 (the first core)\n",
    "p = psutil.Process(os.getpid())\n",
    "p.cpu_affinity([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dc848-0b52-486c-815c-105cb134ae30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecd5e0-992a-442a-8147-b0c3231949be",
   "metadata": {},
   "source": [
    "## Pipeline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e78923-44b4-480c-95ae-4d950255d72e",
   "metadata": {},
   "source": [
    "In this step, we are developing an evaluation strategy that aims to provide a comprehensive assessment of a given machine-learning pipeline by combining both performance metrics and resource usage metrics. Here’s what we’re trying to achieve:\n",
    "\n",
    "1.\t**Performance Metrics Evaluation**:\n",
    "    - **Accuracy and F1 Score**: Measure the model’s ability to correctly classify images.\n",
    "\t- **Confusion Matrix Analysis**: Identify specific classes where the model may be underperforming.\n",
    "2.\t**Resource Utilization Assessment**:\n",
    "\t- **Evaluation Time**: Determine the total time taken for preprocessing and prediction.\n",
    "\t- **Pipeline Size**: Calculate the combined size of the trained model and preprocessing steps to understand storage requirements.\n",
    "\t- **Memory Consumption**: Monitor peak memory usage during evaluation to ensure it fits within hardware constraints.\n",
    "\t- **CPU Usage**: Measure average CPU utilization to evaluate computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c319b04-d065-43a4-87e6-d08f74963912",
   "metadata": {},
   "source": [
    "The primary objective is to not only achieve high classification accuracy but also to assess and optimize the pipeline’s computational efficiency and resource utilization. This holistic approach ensures that the model is not just effective but also practical for deployment, especially in environments with limited computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45240736-9853-4e23-b1d4-4b2add8616a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.pre import evaluate_pipeline # A built-in function to evaluate a given ML pipeline by preprocessing, predicting, and calculating performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc0062-c0a5-4a45-9cc9-83f3318b8644",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "- **model**: The trained machine learning model to evaluate.\n",
    "- **X_test_raw**: Raw test data that needs to be preprocessed before evaluation.\n",
    "- **y_test**: True labels corresponding to the test data for performance comparison.\n",
    "- **preprocessing_fn**: A function used to preprocess the raw test data.\n",
    "    \n",
    "**Outputs:**\n",
    "\n",
    "- **metrics**: A dictionary containing various evaluation metrics like accuracy, F1 score, evaluation time, memory usage, CPU usage, and pipeline size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e340916-7476-4c3a-b2b2-99da9194286f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53499ad4-3fae-47d0-9da7-1c3107e70106",
   "metadata": {},
   "source": [
    "### Preprocessing (Testing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76e2d4-4163-43f8-91f2-1993a229dd13",
   "metadata": {},
   "source": [
    "By applying the same preprocessing to both the training and testing data, we ensure consistent feature representation, which is essential for accurate predictions and prevents errors from data mismatches. This alignment improves model evaluation and generalization to new data. Additionally, combining all preprocessing steps into a single function allows the evaluation function to track execution time and resource usage, ensuring both consistency and computational efficiency across the entire pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e34db3-d518-41e9-815a-58ae0350ba35",
   "metadata": {},
   "source": [
    "### ML method Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9b4dc6-b80d-4b8d-8e33-88391752e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_ML(X): # We apply the same pre-processing steps implemented in Notebook 3.\n",
    "    from skimage.color import rgb2gray\n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    # Normalize the data to [0, 1]\n",
    "    X_pre = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    X_pre = np.array([rgb2gray(image) for image in X_pre])\n",
    "    \n",
    "    # Resize images to 64x64 pixels\n",
    "    X_pre = np.array([resize(image, (64, 64), anti_aliasing=True) for image in X_pre])\n",
    "    \n",
    "    # Flatten the images\n",
    "    num_samples = X_pre.shape[0]\n",
    "    X_pre = X_pre.reshape(num_samples, -1)\n",
    "    \n",
    "    return X_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa180d0-03ce-4097-a03c-3c5c33f9f59f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14307ec6-396a-4ec6-ad20-6ce395f4e73f",
   "metadata": {},
   "source": [
    "### ML Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e390b5-62f3-4d54-9172-49057608ca6b",
   "metadata": {},
   "source": [
    "#### Import data and ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d0bfe1-b3dc-4895-9983-ee7269112b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# first let us load the testing data\n",
    "test_images = np.load('test_images.npy')      # Load image test data\n",
    "test_labels = np.load('test_labels.npy')      # Load label test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f3076-46a8-49ae-bb1e-236634c109d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a97753c-b60d-427e-9ad8-cbd060004273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the ml model from the 3rd notebook\n",
    "with open('sgd_model.pkl', 'rb') as file:\n",
    "    sgd_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5072ad-0d23-416d-8355-f584057d48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(2400,)\n",
      "Evaluation Metrics:\n",
      "evaluation_time: 31.41 seconds\n",
      "peak_memory_usage: 13706.14 MB\n",
      "average_cpu_usage: 99.64%\n",
      "accuracy: 0.4317\n",
      "f1_score: 0.3779\n",
      "confusion_matrix\n",
      "[[ 74   2  42  52 357]\n",
      " [  0 145   1   7   0]\n",
      " [ 32   0  76  38 135]\n",
      " [ 66   0  33  49 394]\n",
      " [ 92   0  54  59 692]]\n",
      "pipeline_size: 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have:\n",
    "# - A trained model named like 'lr_model'\n",
    "# - Raw test data 'X_test_raw'\n",
    "# - True labels 'y_test'\n",
    "# - All pre-processing methods gathered in one function\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(sgd_model, test_images, test_labels, preprocessing_fn_ML)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == 'evaluation_time':\n",
    "        print(f\"{key}: {value:.2f} seconds\")\n",
    "    elif key == 'pipeline_size':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'peak_memory_usage':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'average_cpu_usage':\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    elif key == 'confusion_matrix':\n",
    "        print(key)\n",
    "        print(value) \n",
    "    else:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ba278-d0c4-4f1f-80a9-b7d02ff31f82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512bccf-17e5-4399-8409-edc055906191",
   "metadata": {},
   "source": [
    "### CubeSatNet_CNN Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149f017-0a59-468e-bdcd-188611639bc7",
   "metadata": {},
   "source": [
    "#### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70cc940-e497-4fb6-b86e-801d552a084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_CNN(X):  # we did not use any preprocessing in notebook 4\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bf5a4-2e2d-49da-8369-42d86f654bea",
   "metadata": {},
   "source": [
    "#### Import CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f6a7ac-3fc2-4623-bb3a-809c183c0100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 18:49:54.773744: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-17 18:49:54.779791: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-17 18:49:54.794494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 18:49:54.815749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 18:49:54.822115: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-17 18:49:54.841258: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 18:49:55.963947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Load the CNN model from the 4th notebook\n",
    "with open('cnn_model.pkl', 'rb') as file:\n",
    "    cnn_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537c79e-14cb-495c-adbf-3d6e6d86ca53",
   "metadata": {},
   "source": [
    "#### Evaluate the CNN pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32dc05ff-a71b-4cb4-a991-481e5083cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 3s/step\n",
      "(2400,)\n",
      "(2400,)\n",
      "Evaluation Metrics:\n",
      "evaluation_time: 264.08 seconds\n",
      "peak_memory_usage: 6542.15 MB\n",
      "average_cpu_usage: 88.56%\n",
      "accuracy: 0.9979\n",
      "f1_score: 0.9979\n",
      "confusion_matrix\n",
      "[[524   0   0   3   0]\n",
      " [  0 153   0   0   0]\n",
      " [  0   0 281   0   0]\n",
      " [  2   0   0 540   0]\n",
      " [  0   0   0   0 897]]\n",
      "pipeline_size: 1.16 MB\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "\n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(cnn_model, test_images, test_labels, preprocessing_fn_CNN)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == 'evaluation_time':\n",
    "        print(f\"{key}: {value:.2f} seconds\")\n",
    "    elif key == 'pipeline_size':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'peak_memory_usage':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'average_cpu_usage':\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    elif key == 'confusion_matrix':\n",
    "        print(key)\n",
    "        print(value) \n",
    "    else:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa2a30-1929-42cc-b456-d2ccbacc21a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc715b-c909-41ea-8017-f2a27650323d",
   "metadata": {},
   "source": [
    "### Comparison between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafabda-93ba-4a02-b6ba-822b518ac6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CubeHackKer",
   "language": "python",
   "name": "cubehackker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
